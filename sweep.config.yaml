program: src_model/train.py
method: grid
# metric:
#   name: val_loss
#   goal: minimize

parameters:
  # --- Training Parameters to Sweep ---
  lr:
    value: 1.0e-3
    # distribution: log_uniform_values
    # min: 1.0e-3
    # max: 1.0e-3
  
  batch_size:
    # To maximize batch size without crashing, you need to set the largest possible batch size that fits in GPU memory for each model configuration.
    # This cannot be done statically in YAML; you must implement dynamic batch size selection in your training script.
    # Example: Remove batch_size from sweep and set it in src_model/train.py based on available memory and model size.
    # If you want to sweep over batch sizes, use a range and handle OOM errors in your code:
    value: [512,256]

  # --- Diffusion Parameters to Sweep ---
  timesteps:
    value: 1000
  
  beta_start:
    distribution: log_uniform_values
    min: 5.0e-6
    max: 5.0e-5
  
  beta_end:
    distribution: uniform
    min: 0.001
    max: 0.005

  model:
    parameters:
      base_layer_dim:
        value: 128
      
      map_feat_dim:
        value: 256
      
      robot_feat_dim:
        values: 256
      
      time_feat_dim:
        value: null # Fixed value
      num_internal_layers: 
        values: [3,4]
    
  dynamic:
    value: false

  epochs:
    value: 1500
  
  dataset_path:
    value: "slurm_data/slurm_19_01_8k"
  
  n_maps:
    value: 400
  
  checkpoint_freq:
    value: 250
  
  visualization_freq:
    value: 50

  dropout:
    distribution: uniform
    min: 0.1
    max: 0.4

  weight_decay:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-3

  path_type:
    values: ['extend:-1', 'linear:128','linear:64','cubic:128','cubic:64', 'bspline:10','bspline:15'] # 'linear', 'BSpline', 'cubic' or specify like 'extend:128' to extend to length 128
# early_terminate:
#   type: hyperband
#   min_iter: 300 
#   eta: 2